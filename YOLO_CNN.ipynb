{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO + CNN Classification Project\n",
    "## ×–×™×”×•×™ ××•×‘×™×™×§×˜×™× ×¢× YOLO ×•×¡×™×•×•×’ ×¢× CNN\n",
    "\n",
    "### ××©×™××•×ª:\n",
    "1. ×–×™×”×•×™ ××•×‘×™×™×§×˜ ×¢×™×§×¨×™ ×¢× YOLO\n",
    "2. ×™×¦×™×¨×ª ×“××˜×”×¡×˜ CSV ×¢× ×”×’×‘×œ×” ×œ-10 ×§×˜×’×•×¨×™×•×ª\n",
    "3. ×”×›× ×ª ×“××˜×”×¡×˜ ×œ×œ××™×“×ª ××›×•× ×” (X, Y)\n",
    "4. ×‘× ×™×™×ª ×¨×©×ª CNN\n",
    "5. ×”×¢×¨×›×ª ×”××•×“×œ ×¢×œ validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ×”×ª×§× ×ª ×—×‘×™×œ×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×ª×§× ×ª ×”×—×‘×™×œ×•×ª ×”× ×“×¨×©×•×ª\n",
    "!pip install ultralytics tensorflow scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ××©×™××” 1 - ×–×™×”×•×™ ××•×‘×™×™×§×˜ ×¢×™×§×¨×™ ×¢× YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_main_object_in_images(model, image_dir, output_file):\n",
    "    \"\"\"\n",
    "    ××¨×™×¥ YOLO ×¢×œ ×›×œ ×”×ª××•× ×•×ª ×‘×ª×™×§×™×™×” ×•××–×”×” ××ª ×”××•×‘×™×™×§×˜ ×”×¢×™×§×¨×™\n",
    "    \"\"\"\n",
    "    results_dict = {}\n",
    "    # ×§×‘×œ×ª ×›×œ ×§×‘×¦×™ ×”×ª××•× ×•×ª ×‘×ª×™×§×™×™×”\n",
    "    image_paths = list(Path(image_dir).rglob(\"*.jpg\")) + list(Path(image_dir).rglob(\"*.png\"))\n",
    "\n",
    "    print(f\"××¢×‘×“ {len(image_paths)} ×ª××•× ×•×ª ××ª×™×§×™×™×”: {image_dir}\")\n",
    "\n",
    "    # ×¢×™×‘×•×“ ×›×œ ×ª××•× ×”\n",
    "    for idx, img_path in enumerate(image_paths, 1):\n",
    "        # ×”×“×¤×¡×ª ×”×ª×§×“××•×ª ×›×œ 50 ×ª××•× ×•×ª\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"××¢×•×‘×“×•×ª ×ª××•× ×” {idx}/{len(image_paths)}\")\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # ×”×¨×¦×ª YOLO ×¢×œ ×”×ª××•× ×”\n",
    "            results = model(str(img_path), verbose=False)\n",
    "\n",
    "            # ××¦×™××ª ×”××•×‘×™×™×§×˜ ×”×¢×™×§×¨×™ (×‘×¢×œ ×”-confidence ×”×’×‘×•×” ×‘×™×•×ª×¨)\n",
    "            if len(results) > 0 and len(results[0].boxes) > 0:\n",
    "                boxes = results[0].boxes\n",
    "                confidences = boxes.conf.cpu().numpy()\n",
    "                \n",
    "                if len(confidences) > 0:\n",
    "                    main_obj_idx = confidences.argmax()\n",
    "                    main_class_id = int(boxes.cls[main_obj_idx].cpu().numpy())\n",
    "                    main_confidence = float(confidences[main_obj_idx])\n",
    "                    main_class_name = model.names[main_class_id]\n",
    "\n",
    "                    results_dict[str(img_path)] = {\n",
    "                        \"class_name\": main_class_name,\n",
    "                        \"class_id\": main_class_id,\n",
    "                        \"confidence\": main_confidence,\n",
    "                        \"bbox\": boxes.xyxy[main_obj_idx].cpu().numpy().tolist()\n",
    "                    }\n",
    "                else:\n",
    "                    results_dict[str(img_path)] = {\"class_name\": \"unknown\", \"confidence\": 0.0}\n",
    "            else:\n",
    "                results_dict[str(img_path)] = {\"class_name\": \"unknown\", \"confidence\": 0.0}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            results_dict[str(img_path)] = {\"class_name\": \"unknown\", \"confidence\": 0.0}\n",
    "\n",
    "    # ×©××™×¨×ª ×”×ª×•×¦××•×ª ×œ×§×•×‘×¥ JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"×”×ª×•×¦××•×ª × ×©××¨×• ×‘: {output_file}\")\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×˜×¢×™× ×ª ××•×“×œ YOLO\n",
    "print(\"×˜×•×¢×Ÿ ××•×“×œ YOLO...\")\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# × ×ª×™×‘×™×\n",
    "train_dir = \"resources/images/data/train\"\n",
    "test_dir = \"resources/images/data/test\"\n",
    "val_dir = \"resources/images/data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×¨×¦×” ×¢×œ train\n",
    "print(\"\\n=== ××¢×‘×“ ×ª××•× ×•×ª TRAIN ===\")\n",
    "train_results = detect_main_object_in_images(\n",
    "    yolo_model,\n",
    "    train_dir,\n",
    "    \"train_detections.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×¨×¦×” ×¢×œ test\n",
    "print(\"\\n=== ××¢×‘×“ ×ª××•× ×•×ª TEST ===\")\n",
    "test_results = detect_main_object_in_images(\n",
    "    yolo_model,\n",
    "    test_dir,\n",
    "    \"test_detections.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×¨×¦×” ×¢×œ validation\n",
    "print(\"\\n=== ××¢×‘×“ ×ª××•× ×•×ª VALIDATION ===\")\n",
    "val_results = detect_main_object_in_images(\n",
    "    yolo_model,\n",
    "    val_dir,\n",
    "    \"val_detections.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ××©×™××” 2 - ×™×¦×™×¨×ª CSV ×•×”×’×‘×œ×” ×œ-10 ×§×˜×’×•×¨×™×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_with_top_categories(results_dict, output_csv, max_categories=10):\n",
    "    \"\"\"\n",
    "    ×™×•×¦×¨ CSV ×¢× ××–×”×” ×ª××•× ×” ×•××•×‘×™×™×§×˜ ×¢×™×§×¨×™, ××’×‘×™×œ ×œ-10 ×§×˜×’×•×¨×™×•×ª\n",
    "    \"\"\"\n",
    "    # ×™×¦×™×¨×ª ×¨×©×™××ª ×›×œ ×”××•×‘×™×™×§×˜×™×\n",
    "    data = []\n",
    "    for img_path, result in results_dict.items():\n",
    "        image_id = Path(img_path).stem  # ×©× ×”×§×•×‘×¥ ×œ×œ× ×¡×™×•××ª\n",
    "        class_name = result.get('class_name', 'unknown')\n",
    "        confidence = result.get('confidence', 0.0)\n",
    "        data.append({\n",
    "            'image_id': image_id,\n",
    "            'image_path': img_path,\n",
    "            'detected_object': class_name,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # ×¡×¤×™×¨×ª ×§×˜×’×•×¨×™×•×ª\n",
    "    category_counts = Counter(df['detected_object'])\n",
    "    print(f\"\\n×¡×š ×”×›×œ {len(category_counts)} ×§×˜×’×•×¨×™×•×ª ×©×•× ×•×ª ×–×•×”×•\")\n",
    "    print(\"\\n×”×§×˜×’×•×¨×™×•×ª ×”× ×¤×•×¦×•×ª ×‘×™×•×ª×¨:\")\n",
    "    for cat, count in category_counts.most_common(15):\n",
    "        print(f\"  {cat}: {count}\")\n",
    "    \n",
    "    # ×©××™×¨×ª ×”-9 ×”×§×˜×’×•×¨×™×•×ª ×”× ×¤×•×¦×•×ª ×‘×™×•×ª×¨\n",
    "    top_categories = [cat for cat, _ in category_counts.most_common(max_categories - 1)]\n",
    "    \n",
    "    # ×¡×™×•×•×’ ×›×œ ×“×‘×¨ ××—×¨ ×›-'other'\n",
    "    df['final_category'] = df['detected_object'].apply(\n",
    "        lambda x: x if x in top_categories else 'other'\n",
    "    )\n",
    "    \n",
    "    # ×©××™×¨×ª CSV\n",
    "    df_output = df[['image_id', 'final_category', 'confidence', 'image_path']]\n",
    "    df_output.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nCSV × ×©××¨ ×‘: {output_csv}\")\n",
    "    print(f\"××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª ×¡×•×¤×™: {len(df['final_category'].unique())}\")\n",
    "    print(\"\\n×”×ª×¤×œ×’×•×ª ×§×˜×’×•×¨×™×•×ª ×¡×•×¤×™×ª:\")\n",
    "    print(df['final_category'].value_counts())\n",
    "    \n",
    "    return df, top_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×™×¦×™×¨×ª CSV ×œ×›×œ ×¡×˜\n",
    "print(\"=== Train Dataset ===\")\n",
    "train_df, top_categories = create_csv_with_top_categories(train_results, \"train_labels.csv\")\n",
    "\n",
    "print(\"\\n=== Test Dataset ===\")\n",
    "test_df, _ = create_csv_with_top_categories(test_results, \"test_labels.csv\")\n",
    "\n",
    "print(\"\\n=== Validation Dataset ===\")\n",
    "val_df, _ = create_csv_with_top_categories(val_results, \"val_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ××©×™××” 3 - ×”×›× ×ª ×“××˜×”×¡×˜ ×œ×œ××™×“×ª ××›×•× ×” (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×’×“×¨×•×ª\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, img_height=IMG_HEIGHT, img_width=IMG_WIDTH):\n",
    "    \"\"\"\n",
    "    ×××™×¨ ×ª××•× ×•×ª ×œ××¢×¨×›×™× (X) ×•××›×™×Ÿ ×ª×•×•×™×•×ª (Y)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    print(f\"×˜×•×¢×Ÿ {len(df)} ×ª××•× ×•×ª...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"×˜×¢×•×Ÿ {idx + 1}/{len(df)} ×ª××•× ×•×ª\")\n",
    "        \n",
    "        try:\n",
    "            img_path = row['image_path']\n",
    "            \n",
    "            # ×˜×¢×™× ×ª ×ª××•× ×”\n",
    "            img = load_img(img_path, target_size=(img_height, img_width))\n",
    "            img_array = img_to_array(img)\n",
    "            \n",
    "            # × ×•×¨××œ×™×–×¦×™×” [0, 1]\n",
    "            img_array = img_array / 255.0\n",
    "            \n",
    "            X.append(img_array)\n",
    "            y.append(row['final_category'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    X = np.array(X)\n",
    "    print(f\"\\nX shape: {X.shape}\")\n",
    "    print(f\"Number of labels: {len(y)}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×›× ×ª ×“××˜×”×¡×˜×™×\n",
    "print(\"\\n=== ×”×›× ×ª Train Dataset ===\")\n",
    "X_train, y_train = prepare_dataset(train_df)\n",
    "\n",
    "print(\"\\n=== ×”×›× ×ª Test Dataset ===\")\n",
    "X_test, y_test = prepare_dataset(test_df)\n",
    "\n",
    "print(\"\\n=== ×”×›× ×ª Validation Dataset ===\")\n",
    "X_val, y_val = prepare_dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×™×“×•×“ ×ª×•×•×™×•×ª\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)  # fit ×¢×œ train ×‘×œ×‘×“\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# One-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_cat = to_categorical(y_train_encoded, num_classes)\n",
    "y_test_cat = to_categorical(y_test_encoded, num_classes)\n",
    "y_val_cat = to_categorical(y_val_encoded, num_classes)\n",
    "\n",
    "print(f\"\\n××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª: {num_classes}\")\n",
    "print(f\"×§×˜×’×•×¨×™×•×ª: {label_encoder.classes_}\")\n",
    "print(f\"\\ny_train_cat shape: {y_train_cat.shape}\")\n",
    "print(f\"y_test_cat shape: {y_test_cat.shape}\")\n",
    "print(f\"y_val_cat shape: {y_val_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ××©×™××” 4 - ×‘× ×™×™×ª ×¨×©×ª CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    ×‘× ×™×™×ª ××•×“×œ CNN ××©×•×¤×¨\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×‘× ×™×™×ª ×”××•×“×œ\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "model = build_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# ×”×¦×’×ª ×”××‘× ×”\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×§×•××¤×™×œ×¦×™×” ×¢× ××•×¤×˜×™××™×™×–×¨ ××©×•×¤×¨\n",
    "# × ×©×ª××© ×‘-Adam ×¢× learning rate scheduling\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks ×œ×©×™×¤×•×¨ ×”××™××•×Ÿ\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ××™××•×Ÿ ×”××•×“×œ\n",
    "print(\"\\n=== ××ª×—×™×œ ××™××•×Ÿ ===\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×’×¨×¤×™× ×©×œ ×”××™××•×Ÿ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nG×¨×£ ×”××™××•×Ÿ × ×©××¨ ×‘: training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ××©×™××” 5 - ×”×¢×¨×›×ª ×”××•×“×œ ×¢×œ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×¢×¨×›×” ×¢×œ Train\n",
    "print(\"=== ×”×¢×¨×›×” ×¢×œ Train ===\")\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_cat, verbose=0)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# ×”×¢×¨×›×” ×¢×œ Test\n",
    "print(\"\\n=== ×”×¢×¨×›×” ×¢×œ Test ===\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# ×”×¢×¨×›×” ×¢×œ Validation\n",
    "print(\"\\n=== ×”×¢×¨×›×” ×¢×œ Validation ===\")\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×—×™×–×•×™×™× ×¢×œ Validation\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true_classes = np.argmax(y_val_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title('Confusion Matrix - Validation Set', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix × ×©××¨ ×‘: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n=== Classification Report - Validation Set ===\")\n",
    "print(classification_report(\n",
    "    y_val_true_classes,\n",
    "    y_val_pred_classes,\n",
    "    target_names=label_encoder.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×¡×™×›×•× ×•××¡×§× ×•×ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"                    ×¡×™×›×•× ×ª×•×¦××•×ª                    \")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. ×–×™×”×•×™ ××•×‘×™×™×§×˜×™× ×¢× YOLO:\")\n",
    "print(f\"   - Train: {len(train_results)} ×ª××•× ×•×ª\")\n",
    "print(f\"   - Test: {len(test_results)} ×ª××•× ×•×ª\")\n",
    "print(f\"   - Validation: {len(val_results)} ×ª××•× ×•×ª\")\n",
    "\n",
    "print(f\"\\n2. ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª: {num_classes}\")\n",
    "print(f\"   ×§×˜×’×•×¨×™×•×ª: {', '.join(label_encoder.classes_)}\")\n",
    "\n",
    "print(f\"\\n3. ×’×•×“×œ ×“××˜×”×¡×˜:\")\n",
    "print(f\"   - X_train: {X_train.shape}\")\n",
    "print(f\"   - X_test: {X_test.shape}\")\n",
    "print(f\"   - X_val: {X_val.shape}\")\n",
    "\n",
    "print(f\"\\n4. ×‘×™×¦×•×¢×™ ×”××•×“×œ:\")\n",
    "print(f\"   - Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"   - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"   - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n5. ××¡×§× ×•×ª:\")\n",
    "if train_accuracy - val_accuracy > 0.1:\n",
    "    print(\"   âš ï¸ × ×¨××” ×©×™×© overfitting - ×”×“×™×•×§ ×¢×œ train ×’×‘×•×” ××©××¢×•×ª×™×ª ×-validation\")\n",
    "    print(\"   ğŸ’¡ ××•××œ×¥ ×œ×”×•×¡×™×£ regularization ××• data augmentation\")\n",
    "elif val_accuracy > 0.8:\n",
    "    print(\"   âœ… ×”××•×“×œ ××©×™×’ ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×¢×œ ×”-validation set\")\n",
    "    print(\"   âœ… ×”×“×™×•×§ ××¢×œ 80% ××¢×™×“ ×¢×œ ×¡×™×•×•×’ ××¤×§×˜×™×‘×™\")\n",
    "elif val_accuracy > 0.6:\n",
    "    print(\"   âš ï¸ ×”××•×“×œ ××©×™×’ ×‘×™×¦×•×¢×™× ×‘×™× ×•× ×™×™×\")\n",
    "    print(\"   ğŸ’¡ ××•××œ×¥ ×œ×”×’×“×™×œ ××ª ××¡×¤×¨ ×”-epochs ××• ×œ×©×¤×¨ ××ª ×”××•×“×œ\")\n",
    "else:\n",
    "    print(\"   âŒ ×”××•×“×œ ×–×§×•×§ ×œ×©×™×¤×•×¨ ××©××¢×•×ª×™\")\n",
    "    print(\"   ğŸ’¡ ××•××œ×¥ ×œ×‘×“×•×§ ××ª ××™×›×•×ª ×”×“××˜×” ×•×œ×©×¤×¨ ××ª ××¨×›×™×˜×§×˜×•×¨×ª ×”××•×“×œ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×©××™×¨×ª ×”××•×“×œ ×”×¡×•×¤×™\n",
    "model.save('final_cnn_model.keras')\n",
    "print(\"\\n×”××•×“×œ ×”×¡×•×¤×™ × ×©××¨ ×‘: final_cnn_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
