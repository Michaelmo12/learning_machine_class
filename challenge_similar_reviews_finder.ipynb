{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb465975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "def download_nltk_data():\n",
    "    \"\"\"Download required NLTK data\"\"\"\n",
    "    required_packages = ['punkt', 'punkt_tab', 'stopwords']\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            nltk.data.find(f'tokenizers/{package}' if 'punkt' in package else f'corpora/{package}')\n",
    "        except LookupError:\n",
    "            print(f\"Downloading NLTK {package}...\")\n",
    "            nltk.download(package, quiet=True)\n",
    "\n",
    "# Download NLTK data\n",
    "download_nltk_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Advanced text preprocessing for similarity analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, use_stemming=True, remove_stopwords=True):\n",
    "        self.use_stemming = use_stemming\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "            # Add common hotel review stopwords\n",
    "            self.stop_words.update(['hotel', 'room', 'stay', 'stayed', 'night', 'nights'])\n",
    "        \n",
    "        if use_stemming:\n",
    "            self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace and newlines\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove URLs, emails, phone numbers\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation for sentence boundaries\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
    "        \n",
    "        # Clean up multiple punctuation\n",
    "        text = re.sub(r'[\\.]{2,}', '.', text)\n",
    "        text = re.sub(r'[,]{2,}', ',', text)\n",
    "        text = re.sub(r'[!]{2,}', '!', text)\n",
    "        text = re.sub(r'[?]{2,}', '?', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_and_process(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and process text\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        if not cleaned_text:\n",
    "            return []\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        \n",
    "        # Filter tokens\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Skip punctuation, very short words, and numbers\n",
    "            if (len(token) < 2 or \n",
    "                token in string.punctuation or \n",
    "                token.isdigit()):\n",
    "                continue\n",
    "            \n",
    "            # Remove stopwords if specified\n",
    "            if self.remove_stopwords and token in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Apply stemming if specified\n",
    "            if self.use_stemming:\n",
    "                token = self.stemmer.stem(token)\n",
    "            \n",
    "            processed_tokens.append(token)\n",
    "        \n",
    "        return processed_tokens\n",
    "    \n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"Full preprocessing pipeline returning cleaned text\"\"\"\n",
    "        tokens = self.tokenize_and_process(text)\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ManualTFIDFVectorizer:\n",
    "    \"\"\"Manual implementation of TF-IDF vectorization for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=10000, min_df=2, max_df=0.95):\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.vocabulary = {}\n",
    "        self.idf_values = {}\n",
    "        self.feature_names = []\n",
    "        self.n_documents = 0\n",
    "        \n",
    "    def _compute_tf(self, text_tokens: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute term frequency for a document\"\"\"\n",
    "        tf_dict = {}\n",
    "        total_tokens = len(text_tokens)\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return tf_dict\n",
    "        \n",
    "        token_counts = Counter(text_tokens)\n",
    "        \n",
    "        for word, count in token_counts.items():\n",
    "            tf_dict[word] = count / total_tokens\n",
    "        \n",
    "        return tf_dict\n",
    "    \n",
    "    def _compute_idf(self, documents: List[List[str]]):\n",
    "        \"\"\"Compute inverse document frequency for all terms\"\"\"\n",
    "        # Count document frequency for each term\n",
    "        df_dict = {}\n",
    "        self.n_documents = len(documents)\n",
    "        \n",
    "        for doc_tokens in documents:\n",
    "            unique_tokens = set(doc_tokens)\n",
    "            for token in unique_tokens:\n",
    "                df_dict[token] = df_dict.get(token, 0) + 1\n",
    "        \n",
    "        # Filter terms based on min_df and max_df\n",
    "        filtered_terms = {}\n",
    "        min_df_count = self.min_df if isinstance(self.min_df, int) else int(self.min_df * self.n_documents)\n",
    "        max_df_count = self.max_df if isinstance(self.max_df, int) else int(self.max_df * self.n_documents)\n",
    "        \n",
    "        for term, df in df_dict.items():\n",
    "            if min_df_count <= df <= max_df_count:\n",
    "                filtered_terms[term] = df\n",
    "        \n",
    "        # Sort by frequency and limit features\n",
    "        sorted_terms = sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        if self.max_features and len(sorted_terms) > self.max_features:\n",
    "            sorted_terms = sorted_terms[:self.max_features]\n",
    "        \n",
    "        # Create vocabulary and compute IDF\n",
    "        self.vocabulary = {term: idx for idx, (term, _) in enumerate(sorted_terms)}\n",
    "        self.feature_names = [term for term, _ in sorted_terms]\n",
    "        \n",
    "        for term, df in sorted_terms:\n",
    "            self.idf_values[term] = math.log(self.n_documents / df)\n",
    "    \n",
    "    def fit_transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"Fit the vectorizer and transform documents\"\"\"\n",
    "        # Tokenize all documents\n",
    "        tokenized_docs = []\n",
    "        for doc in documents:\n",
    "            # Simple tokenization (assuming preprocessing is done)\n",
    "            tokens = doc.split() if isinstance(doc, str) else []\n",
    "            tokenized_docs.append(tokens)\n",
    "        \n",
    "        # Compute IDF values\n",
    "        self._compute_idf(tokenized_docs)\n",
    "        \n",
    "        # Transform documents to TF-IDF vectors\n",
    "        return self.transform(documents)\n",
    "    \n",
    "    def transform(self, documents: List[str]) -> np.ndarray:\n",
    "        \"\"\"Transform documents to TF-IDF vectors\"\"\"\n",
    "        tfidf_matrix = np.zeros((len(documents), len(self.vocabulary)))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            tokens = doc.split() if isinstance(doc, str) else []\n",
    "            tf_dict = self._compute_tf(tokens)\n",
    "            \n",
    "            for term, tf in tf_dict.items():\n",
    "                if term in self.vocabulary:\n",
    "                    term_idx = self.vocabulary[term]\n",
    "                    idf = self.idf_values[term]\n",
    "                    tfidf_matrix[doc_idx, term_idx] = tf * idf\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def get_feature_names_out(self) -> List[str]:\n",
    "        \"\"\"Get feature names\"\"\"\n",
    "        return self.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa85a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimilarReviewsFinder:\n",
    "    \"\"\"Main class for finding similar reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, use_sklearn=True, max_features=10000):\n",
    "        self.use_sklearn = use_sklearn\n",
    "        self.max_features = max_features\n",
    "        self.preprocessor = TextPreprocessor(use_stemming=True, remove_stopwords=True)\n",
    "        \n",
    "        if use_sklearn:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),  # Include bigrams\n",
    "                lowercase=True\n",
    "            )\n",
    "        else:\n",
    "            self.vectorizer = ManualTFIDFVectorizer(\n",
    "                max_features=max_features,\n",
    "                min_df=2,\n",
    "                max_df=0.95\n",
    "            )\n",
    "        \n",
    "        self.reviews = []\n",
    "        self.tfidf_matrix = None\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def load_reviews(self, filename: str) -> List[str]:\n",
    "        \"\"\"Load reviews from text file\"\"\"\n",
    "        print(f\"Loading reviews from {filename}...\")\n",
    "        \n",
    "        reviews = []\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if line:  # Skip empty lines\n",
    "                        reviews.append(line)\n",
    "                    \n",
    "                    if line_num % 5000 == 0:\n",
    "                        print(f\"Loaded {line_num} reviews...\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {filename} not found!\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Successfully loaded {len(reviews)} reviews\")\n",
    "        return reviews\n",
    "    \n",
    "    def preprocess_reviews(self, reviews: List[str]) -> List[str]:\n",
    "        \"\"\"Preprocess all reviews\"\"\"\n",
    "        print(\"Preprocessing reviews...\")\n",
    "        processed_reviews = []\n",
    "        \n",
    "        for i, review in enumerate(reviews):\n",
    "            if i % 2000 == 0:\n",
    "                print(f\"Processed {i}/{len(reviews)} reviews\")\n",
    "            \n",
    "            processed = self.preprocessor.preprocess(review)\n",
    "            processed_reviews.append(processed)\n",
    "        \n",
    "        return processed_reviews\n",
    "    \n",
    "    def build_tfidf_matrix(self, reviews: List[str]):\n",
    "        \"\"\"Build TF-IDF matrix from reviews\"\"\"\n",
    "        print(\"Building TF-IDF matrix...\")\n",
    "        \n",
    "        # Preprocess reviews\n",
    "        processed_reviews = self.preprocess_reviews(reviews)\n",
    "        \n",
    "        # Remove empty reviews\n",
    "        valid_reviews = []\n",
    "        valid_indices = []\n",
    "        for i, review in enumerate(processed_reviews):\n",
    "            if review.strip():\n",
    "                valid_reviews.append(review)\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        print(f\"Using {len(valid_reviews)} valid reviews out of {len(reviews)}\")\n",
    "        \n",
    "        # Build TF-IDF matrix\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(valid_reviews)\n",
    "        self.reviews = [reviews[i] for i in valid_indices]  # Keep original reviews\n",
    "        \n",
    "        if hasattr(self.vectorizer, 'get_feature_names_out'):\n",
    "            self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        print(f\"Vocabulary size: {len(self.feature_names)}\")\n",
    "    \n",
    "    def find_similar_reviews(self, query_review: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Find most similar reviews to query\"\"\"\n",
    "        if self.tfidf_matrix is None:\n",
    "            raise ValueError(\"TF-IDF matrix not built. Call build_tfidf_matrix first.\")\n",
    "        \n",
    "        print(f\"\\nFinding {top_k} most similar reviews...\")\n",
    "        print(f\"Query review: {query_review[:100]}...\")\n",
    "        \n",
    "        # Preprocess query\n",
    "        processed_query = self.preprocessor.preprocess(query_review)\n",
    "        if not processed_query.strip():\n",
    "            print(\"Warning: Query review is empty after preprocessing\")\n",
    "            return []\n",
    "        \n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.vectorizer.transform([processed_query])\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get top-k most similar reviews\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            similarity_score = similarities[idx]\n",
    "            original_review = self.reviews[idx]\n",
    "            \n",
    "            result = {\n",
    "                'rank': rank,\n",
    "                'similarity_score': similarity_score,\n",
    "                'review': original_review,\n",
    "                'review_index': idx,\n",
    "                'preview': original_review[:200] + \"...\" if len(original_review) > 200 else original_review\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_query_terms(self, query_review: str, top_terms: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Analyze which terms in query are most important\"\"\"\n",
    "        processed_query = self.preprocessor.preprocess(query_review)\n",
    "        if not processed_query.strip():\n",
    "            return []\n",
    "        \n",
    "        query_vector = self.vectorizer.transform([processed_query])\n",
    "        \n",
    "        # Get non-zero elements (terms present in query)\n",
    "        nonzero_indices = query_vector.nonzero()[1]\n",
    "        term_scores = [(self.feature_names[i], query_vector[0, i]) for i in nonzero_indices]\n",
    "        \n",
    "        # Sort by TF-IDF score\n",
    "        term_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return term_scores[:top_terms]\n",
    "    \n",
    "    def display_results(self, results: List[Dict[str, Any]], query_review: str):\n",
    "        \"\"\"Display search results in a formatted way\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SIMILAR REVIEWS SEARCH RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nQUERY REVIEW:\")\n",
    "        print(f\"{query_review}\\n\")\n",
    "        \n",
    "        # Analyze query terms\n",
    "        important_terms = self.analyze_query_terms(query_review)\n",
    "        if important_terms:\n",
    "            print(\"IMPORTANT QUERY TERMS (TF-IDF scores):\")\n",
    "            for term, score in important_terms:\n",
    "                print(f\"  {term}: {score:.4f}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"MOST SIMILAR REVIEWS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"RANK #{result['rank']} (Similarity: {result['similarity_score']:.4f})\")\n",
    "            print(f\"Review Index: {result['review_index']}\")\n",
    "            print(f\"Review: {result['preview']}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    def get_vocabulary_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the vocabulary\"\"\"\n",
    "        if len(self.feature_names) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Calculate term frequencies across corpus\n",
    "        term_sums = np.array(self.tfidf_matrix.sum(axis=0)).flatten()\n",
    "        term_stats = list(zip(self.feature_names, term_sums))\n",
    "        term_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Handle sparse matrix\n",
    "        if hasattr(self.tfidf_matrix, 'nnz'):  # Sparse matrix\n",
    "            nonzero_count = self.tfidf_matrix.nnz\n",
    "            total_elements = self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1]\n",
    "            sparsity = 1 - (nonzero_count / total_elements)\n",
    "            avg_terms = np.mean(np.diff(self.tfidf_matrix.indptr))\n",
    "        else:  # Dense matrix\n",
    "            nonzero_count = np.count_nonzero(self.tfidf_matrix)\n",
    "            sparsity = 1 - (nonzero_count / self.tfidf_matrix.size)\n",
    "            avg_terms = np.mean(np.count_nonzero(self.tfidf_matrix, axis=1))\n",
    "        \n",
    "        return {\n",
    "            'vocabulary_size': len(self.feature_names),\n",
    "            'total_documents': self.tfidf_matrix.shape[0],\n",
    "            'matrix_sparsity': sparsity,\n",
    "            'top_terms': term_stats[:20],\n",
    "            'avg_terms_per_doc': avg_terms\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate similar reviews finder\"\"\"\n",
    "    \n",
    "    print(\"Similar Reviews Finder\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize finder\n",
    "    finder = SimilarReviewsFinder(use_sklearn=True, max_features=10000)\n",
    "    \n",
    "    # Load reviews\n",
    "    reviews = finder.load_reviews('reviews.txt')\n",
    "    if not reviews:\n",
    "        print(\"No reviews loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Build TF-IDF matrix\n",
    "    finder.build_tfidf_matrix(reviews)\n",
    "    \n",
    "    # Show vocabulary statistics\n",
    "    stats = finder.get_vocabulary_stats()\n",
    "    print(f\"\\nVOCABULARY STATISTICS:\")\n",
    "    print(f\"Vocabulary size: {stats['vocabulary_size']}\")\n",
    "    print(f\"Total documents: {stats['total_documents']}\")\n",
    "    print(f\"Matrix sparsity: {stats['matrix_sparsity']:.4f}\")\n",
    "    print(f\"Average terms per document: {stats['avg_terms_per_doc']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 terms by total TF-IDF score:\")\n",
    "    for term, score in stats['top_terms'][:10]:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "    \n",
    "    # Test with some example queries\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING WITH EXAMPLE QUERIES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    example_queries = [\n",
    "        \"The hotel room was clean and comfortable with excellent service\",\n",
    "        \"Terrible experience, dirty rooms and rude staff\",\n",
    "        \"Great location near the beach, beautiful view from balcony\",\n",
    "        \"The breakfast was amazing and the staff was very helpful\",\n",
    "        \"Expensive parking and noisy rooms, not worth the money\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(example_queries, 1):\n",
    "        print(f\"\\nEXAMPLE {i}:\")\n",
    "        results = finder.find_similar_reviews(query, top_k=3)\n",
    "        finder.display_results(results, query)\n",
    "        \n",
    "        input(\"Press Enter to continue to next example...\")\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE MODE\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Enter your own review to find similar ones (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_query = input(\"\\nEnter review: \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        if not user_query:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            results = finder.find_similar_reviews(user_query, top_k=5)\n",
    "            if results:\n",
    "                finder.display_results(results, user_query)\n",
    "            else:\n",
    "                print(\"No similar reviews found.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\nThank you for using Similar Reviews Finder!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
